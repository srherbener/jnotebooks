{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes, 2/8/18\n",
    "* Netcdf4 and HDF5 both have the same underlying file structure\n",
    "    * File structure is called HDF5\n",
    "* Python Netcdf4\n",
    "    * Lots of unlimited size dimensions needs careful attention\n",
    "        * If more than one unlimited dimension, the default chunk size is 1024\n",
    "            * If this dimension ends up with small size, still have a size of 1024 allocated in the file\n",
    "            * Lots of wasted file space\n",
    "        * Unlimited dimensions appear to be detrimental to file size\n",
    "            * Large reduction in file size can be obtained by minimizing the number of unlimited dimensions\n",
    "        * Variable compression (zlib=True) slows way down with multiple unlimited dimensions\n",
    "        * These effects are likely coming from the need to keep rebuilding the variable as the dimension sizes keep changing\n",
    "            * The data has to be uncompressed and then re-compressed\n",
    "            * The gross mismatch of chunk sizes likely exacerbates this\n",
    "                * Both for performance and memory usage\n",
    "    * Example from jedi_bufr2nc\n",
    "        * jedi_bufr2nc.py Aircraft ../bufr2nc/test/data/gdas.t00z.prepbufr.nr aircraft.test.nc\n",
    "        * The input prepbufr file is 49MB\n",
    "        * When compression (level = 6) was used in jed_bufr2nc.py\n",
    "            * Default chunk sizing\n",
    "            * Process took about 20 minutes to run!\n",
    "            * Output file was about 100MB!\n",
    "            * A variable with size (1867,1) was using chunk size (1024,1024)\n",
    "        * Shut off compression (zlib=False)\n",
    "            * Process much faster --> 2 minutes\n",
    "            * But output file huge, 1GB!\n",
    "                * Ridiculous waste, the output data uncompressed should be around 350KB\n",
    "        * Specified chunksize using a size of 1 for all unlimited dimensions\n",
    "            * Runtime about the same\n",
    "            * File size reduced to 86MB\n",
    "                * Way better, but still excessive waste\n",
    "        * Used nccopy for two more improvements\n",
    "            * Change unlimited dims to fixed dims\n",
    "                * nccopy -u infile outfile\n",
    "            * Shuffle and compress file (level 6)\n",
    "                * nccopy -d 6 -s infile outfile\n",
    "            * Recommended to do these in two distict steps\n",
    "                * compression works much more effectively when dims are fixed\n",
    "        * Change unlimited dims to fixed dims\n",
    "            * Long runtime: 5 - 10 minutes\n",
    "            * File reduced to 6MB\n",
    "            * Note: no compress has been applied at this point\n",
    "        * Shuffle and compress file\n",
    "            * Very fast, 1 second\n",
    "            * File reduce to 211KB\n",
    "                * Much more reasonable\n",
    "        * Summary of impacts show in sequence the actions were tried\n",
    "        \n",
    "| Action | File Size | Var Size | Chunk Size |\n",
    "|:-------|:---------:|:--------:|:----------:|\n",
    "|Default chunking|1GB|(1867,1)|(1024,1024)|\n",
    "|Chunking with size 1 for unlim dims|86MB|(1867,1)|(1,1)|\n",
    "|Change unlim dims to fixed dims|6MB|(1867,1)|(1867,1)|\n",
    "|Shuffle and compress|221KB|(1861,1)|(1861,1)|\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notes, 2/9/18\n",
    "\n",
    "## jedi_bufr2nc.py\n",
    "\n",
    "* Cannot directly query the BUFR file (nor BUFR table) to find dimension sizes\n",
    "* Options for efficiently writing the netCDF file\n",
    "    * Two passes through the BUFR file\n",
    "        * Pass1: read all obs and determine dimension sizes\n",
    "            * Won't work to read a representative obs since number of levels can be different (T vs U example)\n",
    "        * Pass2: read obs and transfer to the netcdf file\n",
    "        \n",
    "        * Pros:\n",
    "            * Will get dimension sizes set to the minimum necessary per file\n",
    "            * Can make all netcdf dimensions fixed size\n",
    "            \n",
    "        * Cons:\n",
    "            * Slow to read the BUFR file twice (but not that bad)\n",
    "            \n",
    "    * One pass through the BUFR file and post process the netCDF file\n",
    "        * Create file uncompressed and unlimited dimensions while reading the BUFR file\n",
    "        * Convert dims to fixed\n",
    "        * Compress\n",
    "        \n",
    "        * Pros: \n",
    "            * Save time by only reading BUFR file once\n",
    "            * Compression may be optimized since the variables are complete before compression starts\n",
    "                * May not make much difference\n",
    "        \n",
    "        * Cons:\n",
    "            * Slow\n",
    "                * The \"convert dims to fixed\" is an especially slow process\n",
    "\n",
    "    * One pass through the BUFR file and ssume max dimensions for the netCDF file\n",
    "        * Pros:\n",
    "            * Fastest execution\n",
    "        \n",
    "        * Cons:\n",
    "            * Wastes space\n",
    "                * Compression may mitigate this since there will be a lot of repeated values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notes, 2/12/18\n",
    "\n",
    "## jedi_bufr2nc.py\n",
    "\n",
    "* Created srherbener/bufr2nc repository on GitHub\n",
    "    * jedi_bufr2nc.py script\n",
    "    * Two Fortran utilities\n",
    "        * pb_decode.f90: dump out bufr table and obs (ufbint() calls)\n",
    "        * pb_decode_events.f90: dump out bufr table and events (ufbevn() calls)\n",
    "* Test case: gdas.t00z.prepbufr.nr\n",
    "    * Runs in about 30 seconds\n",
    "    * Input 49MB\n",
    "    * Output (AIRCFT, AIRCAR) 200KB\n",
    "        * 18 messages selected\n",
    "        * 1867 obs recorded\n",
    "* Xin's file: prepbufr.gdas.20160304.t06z.nr.48h\n",
    "    * Runs in 1.5 hours\n",
    "    * Input 62MB\n",
    "    * Output (AIRCFT, AIRCAR) 4.3MB\n",
    "        * 2268 messages selected\n",
    "        * 222316 obs recorded\n",
    "* Performance is not good, the Fortran programs run a little faster considering the python script makes two passes through the BUFR file.\n",
    "    * Reading obs from a subset is slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
